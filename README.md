# Taxi Data Processing Platform

Há»‡ thá»‘ng xá»­ lÃ½ vÃ  phÃ¢n tÃ­ch dá»¯ liá»‡u taxi sá»­ dá»¥ng kiáº¿n trÃºc Lambda Architecture, káº¿t há»£p batch processing vÃ  stream processing.

## ğŸ‘¥ ThÃ nh viÃªn nhÃ³m

| STT | Há» vÃ  tÃªn | MSSV |
|-----|-----------|------|
| 1   | DÆ°Æ¡ng VÅ© HoÃ ng | 20224987 |
| 2   | LÃª Nháº­t HoÃ ng | 20224989 |
| 3   | Nguyá»…n ThÃ nh Trung | 20225105 |
| 4   | Chu Tuáº¥n NghÄ©a | 20225056 |

## ğŸ—ï¸ Kiáº¿n trÃºc há»‡ thá»‘ng

Há»‡ thá»‘ng Ä‘Æ°á»£c xÃ¢y dá»±ng dá»±a trÃªn **Lambda Architecture** vá»›i 3 táº§ng chÃ­nh:

- **Batch Layer**: Xá»­ lÃ½ dá»¯ liá»‡u lá»›n Ä‘á»‹nh ká»³ vá»›i Apache Spark vÃ  Dagster
- **Speed Layer**: Xá»­ lÃ½ dá»¯ liá»‡u streaming real-time vá»›i Kafka vÃ  Spark Streaming
- **Serving Layer**: Truy váº¥n vÃ  visualize dá»¯ liá»‡u vá»›i Trino, ClickHouse vÃ  Superset

## ğŸ› ï¸ CÃ´ng nghá»‡ sá»­ dá»¥ng

### Data Storage
- **HDFS**: Distributed file system cho batch data
- **ClickHouse**: OLAP database cho analytical queries
- **MongoDB**: Document store cho metadata

### Data Processing
- **Apache Spark**: Batch vÃ  stream processing engine
- **Dagster**: Orchestration cho batch jobs
- **Kafka**: Message broker cho data streaming
- **Kafka Connect**: Data integration framework

### Query & Visualization
- **Trino**: Distributed SQL query engine
- **Apache Superset**: Data visualization vÃ  dashboarding

### Infrastructure
- **Kubernetes**: Container orchestration
- **Minikube**: Local Kubernetes cluster
- **Docker**: Containerization

## ğŸ“ Cáº¥u trÃºc thÆ° má»¥c

```
it4931_project/
â”‚
â”‚
â”œâ”€â”€ ğŸ³ docker_infra/                  # Docker infrastructure
â”œâ”€â”€ â˜¸ï¸  k8s/                           # Kubernetes manifests
â”‚   â”œâ”€â”€ clickhouse/                   # ClickHouse deployment
â”‚   â”œâ”€â”€ dagster/                      # Dagster deployment
â”‚   â”œâ”€â”€ hdfs/                         # HDFS deployment
â”‚   â”œâ”€â”€ kafka/                        # Kafka cluster deployment
â”‚   â”œâ”€â”€ mongodb/                      # MongoDB deployment
â”‚   â”œâ”€â”€ spark/                        # Spark cluster deployment
â”‚   â”œâ”€â”€ superset/                     # Superset deployment
â”‚   â””â”€â”€ trino/                        # Trino deployment
â”‚
â”œâ”€â”€ ğŸ”§ setup/                         # Setup scripts
â”‚   â”œâ”€â”€ 01-build-and-load-images.sh   # Build vÃ  load Docker images
â”‚   â”œâ”€â”€ 02-deploy-services.sh         # Deploy K8s services
â”‚   â”œâ”€â”€ 03-setup-hdfs.sh              # Khá»Ÿi táº¡o HDFS
â”‚   â”œâ”€â”€ 04-setup-kafka.sh             # Khá»Ÿi táº¡o Kafka topics
â”‚   â”œâ”€â”€ 05-setup-clickhouse.sh        # Khá»Ÿi táº¡o ClickHouse
â”‚   â”œâ”€â”€ 06-setup-mongo.sh             # Khá»Ÿi táº¡o MongoDB
â”‚   â””â”€â”€ 07-setup-superset.sh          # Khá»Ÿi táº¡o Superset
â”‚
â”œâ”€â”€ ğŸ’» src/                           # Source code
â”‚   â”œâ”€â”€ ingestion/                    # Data ingestion
â”‚   â”‚   â””â”€â”€ nghiact_producer.py       # Kafka producer
â”‚   â”œâ”€â”€ spark_jobs/                   # Batch processing jobs
â”‚   â”‚   â”œâ”€â”€ batch_job.py              # Main batch job
â”‚   â”‚   â””â”€â”€ star_schema_transform.py  # Transform to star schema
â”‚   â””â”€â”€ stream_job/                   # Stream processing jobs
â”‚       â””â”€â”€ streaming_job.py          # Spark streaming job
â”œâ”€â”€ ğŸ“ requirements.txt               # Python dependencies
â”œâ”€â”€ ğŸ³ Dockerfile.spark               # Spark Docker image
â””â”€â”€ ğŸ“– README.md                      # This file
```

## ğŸš€ HÆ°á»›ng dáº«n cÃ i Ä‘áº·t vÃ  triá»ƒn khai

### YÃªu cáº§u há»‡ thá»‘ng

- **Minikube** >= 1.30
- **kubectl** >= 1.27
- **Docker** >= 20.10
- **Python** >= 3.9
- RAM: Tá»‘i thiá»ƒu 16GB (khuyáº¿n nghá»‹ 32GB)
- CPU: Tá»‘i thiá»ƒu 4 cores (khuyáº¿n nghá»‹ 8 cores)

### ğŸ“¦ BÆ°á»›c 1: Khá»Ÿi Ä‘á»™ng Minikube

Khá»Ÿi Ä‘á»™ng Minikube vá»›i tÃ i nguyÃªn tá»‘i Ä‘a:

```bash
minikube start --cpus=max --memory=max
```

> **LÆ°u Ã½**: Báº¡n cÃ³ thá»ƒ Ä‘iá»u chá»‰nh sá»‘ lÆ°á»£ng CPU vÃ  memory theo nhu cáº§u:
> ```bash
> minikube start --cpus=8 --memory=16384
> ```

### ğŸ”§ BÆ°á»›c 2: CÃ i Ä‘áº·t cÃ¡c thÃ nh pháº§n

Cháº¡y cÃ¡c script setup theo thá»© tá»±:

```bash
# 1. Build vÃ  load Docker images vÃ o Minikube
bash ./setup/01-build-and-load-images.sh

# 2. Deploy táº¥t cáº£ services lÃªn Kubernetes
bash ./setup/02-deploy-services.sh

# 3. Khá»Ÿi táº¡o HDFS vÃ  upload dá»¯ liá»‡u
bash ./setup/03-setup-hdfs.sh

# 4. Táº¡o Kafka topics vÃ  deploy connectors
bash ./setup/04-setup-kafka.sh

# 5. Khá»Ÿi táº¡o ClickHouse database vÃ  tables
bash ./setup/05-setup-clickhouse.sh

# 6. Khá»Ÿi táº¡o MongoDB collections
bash ./setup/06-setup-mongo.sh

# 7. Khá»Ÿi táº¡o Superset admin user vÃ  dashboards
bash ./setup/07-setup-superset.sh
```

### ğŸŒ BÆ°á»›c 3: Port Forwarding

Äá»ƒ truy cáº­p cÃ¡c services tá»« mÃ¡y local, má»Ÿ terminal má»›i vÃ  cháº¡y:

```bash
# Forward Kafka brokers
kubectl port-forward k-broker-0 9094:9095 &
kubectl port-forward k-broker-1 9095:9095 &
```

### ğŸ” Láº¥y Ä‘á»‹a chá»‰ IP cá»§a Minikube

```bash
minikube ip
```

Ghi láº¡i Ä‘á»‹a chá»‰ IP nÃ y Ä‘á»ƒ sá»­ dá»¥ng khi káº¿t ná»‘i Ä‘áº¿n cÃ¡c services. ThÆ°á»ng sáº½ lÃ  192.168.49.2

## ğŸ¯ HÆ°á»›ng dáº«n sá»­ dá»¥ng

### 1ï¸âƒ£ Cháº¡y Data Producer (Speed Layer)

Giáº£ láº­p luá»“ng dá»¯ liá»‡u taxi real-time vÃ o Kafka:

```bash
python ./src/ingestion/nghiact_producer.py
```

Producer sáº½ Ä‘á»c dá»¯ liá»‡u tá»« Parquet files vÃ  gá»­i vÃ o Kafka topic vá»›i tá»‘c Ä‘á»™ Ä‘iá»u chá»‰nh Ä‘Æ°á»£c.
Quan sÃ¡t Kafka UI táº¡i http://192.168.49.2:30808

### 2ï¸âƒ£ Submit Spark Streaming Job

Xá»­ lÃ½ dá»¯ liá»‡u streaming tá»« Kafka:

```bash
kubectl exec $(kubectl get pods -l app=spark-master -o jsonpath='{.items[0].metadata.name}') -it -- \
    spark-submit ./src/stream_job/streaming_job.py
```

Streaming job sáº½:
- Äá»c dá»¯ liá»‡u tá»« Kafka
- Transform vÃ  enrich dá»¯ liá»‡u
- Ghi káº¿t quáº£ vÃ o ClickHouse

### 3ï¸âƒ£ Cháº¡y Batch Job (Batch Layer)

Truy cáº­p Dagster UI Ä‘á»ƒ trigger batch jobs:

1. Má»Ÿ trÃ¬nh duyá»‡t: http://192.168.49.2:30300

2. Chá»n job vÃ  click **"Launchpad"** Ä‘á»ƒ cháº¡y

### 4ï¸âƒ£ Visualize dá»¯ liá»‡u vá»›i Superset

#### Káº¿t ná»‘i Superset vá»›i Trino:

1. Truy cáº­p http://192.168.49.2:30088
   - Username: `admin`
   - Password: `admin`

2. ThÃªm Trino connection:
   - **Database**: Apache Superset
   - **SQLAlchemy URI**: `trino://trino@trino-service:8080/`

3. Táº¡o datasets vÃ  charts tá»« Trino queries

